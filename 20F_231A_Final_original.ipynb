{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5MTFOH5kUN2"
   },
   "source": [
    "# 2020 Fall STAT 231A --- Final Deep Q-Network (DQN)\n",
    "---\n",
    "In this notebook, you will try two reinforcement learning algorithm : \n",
    "1. Deep Q-learning with replay buffer. \n",
    "2. Policy gradient.\n",
    "\n",
    "on OpenAI Gym's Atari/box2d game. \n",
    "\n",
    "I provided all the code necessary. What you have to do is modify the corresponding network structure and hyperparameters. The current network structure are defined to run the game \"CartPole-v0\", which is the easiest game in GYM. A very good official pytorch tutorial is a good start. https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html. You are required to choose at least one of the following games. You can choose any atari / box2d game you like under this two webpage: \n",
    "\n",
    "- [EASY] https://gym.openai.com/envs/#box2d The box2d game state is the smallest. e.g. LunarLander-V2, it has only 8 dims.\n",
    "\n",
    "- https://gym.openai.com/envs/#atari Each atari game has two kind of input. \n",
    "    - [MEDIUM] RAM version has a small state of only 128 dims. You can use fully connected layer to train.\n",
    "    - [HELL] Screen version takes image as state which is around 200\\*200\\*3 dims. You need conv layer to train.\n",
    "\n",
    "The implementation of [EASY] is required. If you make it all right, typical you will train a good agent within 1000 epochs. \n",
    "[MEDIUM] and [HELL] is optional with bouns. Challange your self on atari game. Screen version need CNN and typically need 10 hour to train.\n",
    "\n",
    "You have to \"solve\" the problem to earn full credits. Definition of solved : See https://github.com/openai/gym/wiki/Leaderboard\n",
    "\n",
    "There are no specific definition of solved for atari game.\n",
    "\n",
    "Upload two files for coding part in Final.\n",
    "\n",
    "- A pdf files : Your report. Please write down specific algorithm, implementing detail and result (Include sample game screenshot and reward-epoch plot) Also, attach all the code at the end of the pdf. For implementing detail, you can just comment on the code. \n",
    "\n",
    "- This ipynb files.\n",
    "\n",
    "- PS. If you think my implementation is bad, fell free to implement your own. You can use Tensorflow if you prefer to do so. However, please define the same class as this template. Include at least : agent class with act and learn; replay class with push and sample; q-function class with deep network structure; a train function.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yc3iQJZDEqN3"
   },
   "outputs": [],
   "source": [
    "!pip install box2d-py\n",
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9r7Pi_sW2CL"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "%matplotlib inline\n",
    "\n",
    "def show_video(folder):\n",
    "    mp4list = glob.glob('%s/*.mp4' % folder)\n",
    "    if len(mp4list) > 0:\n",
    "        encoded = base64.b64encode(io.open(mp4list[0], 'r+b').read())\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\"> \n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /> </video>'''.format(encoded.decode('ascii'))))\n",
    "        \n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zad92JBxkUN6"
   },
   "source": [
    "### 2. Try it\n",
    "\n",
    "The following code will output a sample video whose action is random sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ff6NvOh5kUN7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# atari_game = \"Breakout-ram-v0\"\n",
    "# atari_game = \"LunarLander-v2\"\n",
    "atari_game = \"CartPole-v0\"\n",
    "env = gym.wrappers.Monitor(gym.make(atari_game), 'sample', force=True)\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)\n",
    "\n",
    "state = env.reset()\n",
    "cr = 0\n",
    "for j in range(2000):\n",
    "    action = env.action_space.sample()\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cr += reward\n",
    "    print('\\r %.5f' % cr, end=\"\")\n",
    "    if done:\n",
    "        break \n",
    "env.close()\n",
    "show_video('sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjIHJgWzkUOG"
   },
   "source": [
    "### 3. Define QNetwork, agent and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xi6Z4RGFkaIM"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 1e-3               # learning rate \n",
    "UPDATE_EVERY = 5        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=256, fc3_units=256, fc4_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # self.seed   = torch.manual_seed(seed)\n",
    "        # self.state_size  = state_size\n",
    "        # self.action_size = action_size\n",
    "        # self.fc1_units   = fc1_units\n",
    "        # self.fc2_units   = fc2_units\n",
    "        # self.fc3_units   = fc3_units\n",
    "        # self.fc4_units   = fc4_units\n",
    "        # self.layer1 = nn.Linear(self.state_size, self.fc1_units, bias=True)\n",
    "        # self.bn1 = nn.BatchNorm1d(self.fc1_units)\n",
    "        # self.dp1 = nn.Dropout(p=0.5)\n",
    "        # self.layer2 = nn.Linear(self.fc1_units,  self.fc2_units, bias=True)\n",
    "        # self.bn2 = nn.BatchNorm1d(self.fc2_units)\n",
    "        # self.dp2 = nn.Dropout(p=0.5)\n",
    "        # self.layer3 = nn.Linear(self.fc2_units,  self.fc3_units, bias=True)\n",
    "        # self.bn3 = nn.BatchNorm1d(self.fc3_units)\n",
    "        # self.dp3 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # self.layer4 = nn.Linear(self.fc3_units,  self.fc4_units, bias=True)\n",
    "        # self.layer5 = nn.Linear(self.fc4_units,  self.action_size, bias=True)\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "\n",
    "        return state\n",
    "\n",
    "        # layer1 = F.relu(self.layer1(state))\n",
    "        \n",
    "        # layer2 = F.relu(self.layer2(layer1))\n",
    "        # layer3 = F.relu(self.layer3(layer2))\n",
    "        # layer4 = F.relu(self.layer4(layer3))\n",
    "        # layer5 = self.layer5(layer4)\n",
    "        # return layer5\n",
    "        \n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)  \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "               \n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyEOvOeakUOP"
   },
   "source": [
    "### 3. Train the Agent with DQN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEHrq9I18gqX"
   },
   "outputs": [],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    env = gym.wrappers.Monitor(gym.make(atari_game), 'output', force=True)\n",
    "    render = True\n",
    "    for i_episode in range(0, n_episodes):\n",
    "        if render and i_episode % 100 == 0:\n",
    "            env = gym.wrappers.Monitor(gym.make(atari_game), 'output_%d' % i_episode, force=True)\n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            if t%100==0:\n",
    "                action = 1\n",
    "            if render and i_episode % 100 == 0:\n",
    "                env.render()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if render:\n",
    "                env.close()\n",
    "                show_video('output_%d' % i_episode)\n",
    "                env = gym.make(atari_game)\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "\n",
    "agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0)\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_fodh-ZHcTM"
   },
   "source": [
    "You can load the parameter by this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nk7IhMVgvPwX"
   },
   "outputs": [],
   "source": [
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient \n",
    "\n",
    "This one is implemented in pure Python. \n",
    "\n",
    "### Define PG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "atari_game = \"CartPole-v0\"\n",
    "env = gym.make(atari_game)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LogisticPolicy:\n",
    "    \n",
    "    def __init__(self, θ, α, γ):\n",
    "        # Initialize paramters θ, learning rate α and discount factor γ\n",
    "        \n",
    "        self.θ = θ\n",
    "        self.α = α\n",
    "        self.γ = γ\n",
    "        \n",
    "    def logistic(self, y):\n",
    "        # definition of logistic function\n",
    "        \n",
    "        return 1/(1 + np.exp(-y))\n",
    "    \n",
    "    def probs(self, x):\n",
    "        # returns probabilities of two actions\n",
    "        \n",
    "        y = x @ self.θ\n",
    "        prob0 = self.logistic(y)\n",
    "        \n",
    "        return np.array([prob0, 1-prob0])        \n",
    "    \n",
    "    def act(self, x):\n",
    "        # sample an action in proportion to probabilities\n",
    "        \n",
    "        probs = self.probs(x)\n",
    "        action = np.random.choice([0, 1], p=probs)\n",
    "        \n",
    "        return action, probs[action]\n",
    "    \n",
    "    def grad_log_p(self, x):\n",
    "        # calculate grad-log-probs\n",
    "        \n",
    "        y = x @ self.θ        \n",
    "        grad_log_p0 = x - x*self.logistic(y)\n",
    "        grad_log_p1 = - x*self.logistic(y)\n",
    "        \n",
    "        return grad_log_p0, grad_log_p1\n",
    "        \n",
    "    def grad_log_p_dot_rewards(self, grad_log_p, actions, discounted_rewards):\n",
    "        # dot grads with future rewards for each action in episode\n",
    "        \n",
    "        return grad_log_p.T @ discounted_rewards\n",
    "    \n",
    "    def discount_rewards(self, rewards):\n",
    "        # calculate temporally adjusted, discounted rewards\n",
    "        \n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(0, len(rewards))):\n",
    "            cumulative_rewards = cumulative_rewards * self.γ + rewards[i]\n",
    "            discounted_rewards[i] = cumulative_rewards\n",
    "            \n",
    "        return discounted_rewards\n",
    "\n",
    "    def update(self, rewards, obs, actions):\n",
    "        # calculate gradients for each action over all observations\n",
    "        grad_log_p = np.array([self.grad_log_p(ob)[action] for ob,action in zip(obs,actions)])\n",
    "        \n",
    "        assert grad_log_p.shape == (len(obs), 4)\n",
    "        \n",
    "        # calculate temporaly adjusted, discounted rewards\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "        \n",
    "        # gradients times rewards\n",
    "        dot = self.grad_log_p_dot_rewards(grad_log_p, actions, discounted_rewards)\n",
    "        \n",
    "        # gradient ascent on parameters\n",
    "        self.θ += self.α*dot\n",
    "    \n",
    "def run_episode(env, policy, render=False):\n",
    "    \n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    \n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        observations.append(observation)\n",
    "        \n",
    "        action, prob = policy.act(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        totalreward += reward\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        probs.append(prob)\n",
    "    \n",
    "    return totalreward, np.array(rewards), np.array(observations), np.array(actions), np.array(probs)\n",
    "\n",
    "def train(θ, α, γ, Policy, MAX_EPISODES=1000, seed=None, evaluate=False):\n",
    "    \n",
    "    # initialize environment and policy\n",
    "    env = gym.make('CartPole-v0')\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    episode_rewards = []\n",
    "    policy = Policy(θ, α, γ)\n",
    "    \n",
    "    # train until MAX_EPISODES\n",
    "    for i in range(MAX_EPISODES):\n",
    "\n",
    "        # run a single episode\n",
    "        total_reward, rewards, observations, actions, probs = run_episode(env, policy)\n",
    "                \n",
    "        # keep track of episode rewards\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # update policy\n",
    "        policy.update(rewards, observations, actions)\n",
    "        print(\"EP: \" + str(i) + \" Score: \" + str(total_reward) + \" \",end=\"\\r\", flush=False) \n",
    "\n",
    "    # evaluation call after training is finished - evaluate last trained policy on 100 episodes\n",
    "    if evaluate:\n",
    "        env = Monitor(env, 'pg_cartpole/', video_callable=False, force=True)\n",
    "        for _ in range(100):\n",
    "            run_episode(env, policy, render=False)\n",
    "        env.env.close()\n",
    "        \n",
    "    return episode_rewards, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.monitor import Monitor, load_results\n",
    "\n",
    "# for reproducibility\n",
    "GLOBAL_SEED = 0\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "episode_rewards, policy = train(θ=np.random.rand(4),\n",
    "                                α=0.002,\n",
    "                                γ=0.99,\n",
    "                                Policy=LogisticPolicy,\n",
    "                                MAX_EPISODES=2000,\n",
    "                                seed=GLOBAL_SEED,\n",
    "                                evaluate=True)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(episode_rewards);\n",
    "results = load_results('pg_cartpole')\n",
    "plt.hist(results['episode_rewards'], bins=20);"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "231BFinal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
